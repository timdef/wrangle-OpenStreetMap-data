{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Project\n",
    "\n",
    "\n",
    "*created by: Tim Urian* \n",
    "<tim.urian@gmail.com>\n",
    "\n",
    "\n",
    "In this project I downloaded 5+ GB of [OpenStreetMap](https://www.openstreetmap.org) data from around the [Los Angeles Metro Area](https://mapzen.com/data/metro-extracts/metro/los-angeles_california/).  \n",
    "\n",
    "I currently live in the Los Angeles and have gotten to meet some of the people who update and maintain OSM data at tech meetups around the city. I was excited to play with a much bigger dataset than Iâ€™ve dealt with before, and to get more experience with SQL. \n",
    "\n",
    "In the first section of this project I clean the data and create csv's to be imported into an SQLite database using python. The data is parsed iteratively to deal with the large filesize.\n",
    "\n",
    "The issues in the data I addressed included:\n",
    "* __Streetnames__\n",
    " * Poorly Capitalized \n",
    " * Overly Abbreviated \n",
    " \n",
    " \n",
    "* __Zip Codes__\n",
    " * Standardized to 5 digits\n",
    " * Contained extra spaces, characters\n",
    " * Eliminated anything that wasn't a zipcode\n",
    " * Eliminated zips not in California\n",
    " * Some began with 'CA'\n",
    "\n",
    "Some of the problems were issues I expected (the +4 zip codes for example), others were found in a circular process of normalizing data with the most glaring problems, then seeing what issues I had missed and whether or not they could be solved programmatically. \n",
    "\n",
    "In the second section of this document I use SQL to explore the dataset and answer some questions that interest me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Processing the Data\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "##################\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "import string\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "##################\n",
    "\n",
    "NODE_COLUMNS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "TAG_COLUMNS = ['id', 'key', 'value', 'type']\n",
    "WAY_COLUMNS = ['id', 'user', 'uid', 'version', 'timestamp', 'changeset']\n",
    "WAY_NODES_COLUMNS = ['id', 'node_id', 'position']\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "SCHEMA_DICT = {'node': NODE_COLUMNS, \n",
    "               'tag': TAG_COLUMNS,\n",
    "               'way': WAY_COLUMNS,\n",
    "               'nd': WAY_NODES_COLUMNS}\n",
    "\n",
    "filename ='los-angeles_california.osm'\n",
    "logging.basicConfig(filename='parsing.log', level=logging.INFO)\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "################\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way')):\n",
    "    \"\"\"Yield Element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, element in context:\n",
    "        if event == 'end' and element.tag in tags:\n",
    "            yield element\n",
    "            root.clear()\n",
    "\n",
    "def format_tag_key(tag, key='key'):\n",
    "    \"\"\"Returns seperate, corrected key and type values\"\"\"\n",
    "    if PROBLEMCHARS.search(tag[key]) is None:\n",
    "        if bool(LOWER_COLON.search(tag[key])):\n",
    "            split_tag = tag[key].split(':', 1)\n",
    "            tag[key] = split_tag[1]\n",
    "            tag['type'] = split_tag[0]\n",
    "        else:\n",
    "            tag['type'] = 'regular'\n",
    "    else:\n",
    "        logging.info('Tag has problem character: %s skipping.', repr(tag[key]))\n",
    "        tag['type'] = 'skip'\n",
    "    return tag\n",
    "\n",
    "def new_shape_element(element):\n",
    "    \"\"\"Returns dict correctly shaped for writing to csv file\"\"\"\n",
    "    row = {}\n",
    "    tags = []\n",
    "    nds= []\n",
    "    \n",
    "    for attribute in SCHEMA_DICT[element.tag]:\n",
    "        if attribute in element.attrib.keys():\n",
    "            row[attribute] = element.attrib[attribute]\n",
    "    shaped_dict = {element.tag : row}\n",
    "    \n",
    "    for tag in element.iter('tag'):\n",
    "        tag_as_dict = {'id':row['id'], 'key':tag.attrib['k'], 'value':tag.attrib['v'], 'type':'regular'}\n",
    "        tag_as_dict = format_tag_key(tag_as_dict)\n",
    "        if tag_as_dict['type'] != 'skip':\n",
    "            tags.append(tag_as_dict)\n",
    "    if tags:\n",
    "        shaped_dict['tags'] = tags\n",
    "    \n",
    "    for position, nd in enumerate(element.iter('nd')):\n",
    "        nd_as_dict = {'id':row['id'], 'node_id':nd.attrib['ref'], 'position':position}\n",
    "        nds.append(nd_as_dict)\n",
    "    if nds:\n",
    "        shaped_dict['way_nodes'] = nds\n",
    "    \n",
    "    return shaped_dict\n",
    "\n",
    "\n",
    "def add_row_to_csv(filename, row, fieldnames):\n",
    "    with open(filename, 'wb') as f:\n",
    "        writer = csv.DictWriter(sys.stderr, fieldnames=fieldnames)\n",
    "        writer.writerow(row)\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Auditing Functions \n",
    "##########################\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "is_postcode_re = re.compile('^\\d{5}$')\n",
    "is_postcode_plus_four_re = re.compile(r'.*(\\d{5}(\\-\\d{4}))$')\n",
    "\n",
    "expected_streetnames = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Highway\", \"Pike\", \"Way\", \"Circle\", \"Terrace\", \"Alley\",\"Canal\",\"Center\",\"Circle\",\n",
    "                       \"Cove\", \"Trail\", \"Way\" ]\n",
    "streetname_mapping = {'Al':'Alley',\n",
    "                     'Ave':'Avenue',\n",
    "                     'Av':'Avenue',\n",
    "                     'Blvd':'Boulevard',\n",
    "                     'Bv':'Boulevard',\n",
    "                      'Boulvard':'Boulevard',\n",
    "                      'Ca':'Canal',\n",
    "                      'Cn':'Center',\n",
    "                      'Cr':'Cirle',\n",
    "                      'Cir':'Circle',\n",
    "                      'Ct':'Court',\n",
    "                      'Cv':'Cove',\n",
    "                      'Dr': 'Drive',\n",
    "                      'Hwy':'Highway',\n",
    "                     'Pkwy': 'Parkway',\n",
    "                      'Pky':'Parkway',\n",
    "                      'Pl':'Place',\n",
    "                     'Rd':'Road',\n",
    "                     'St':'Street',\n",
    "                      'Sq':'Square',\n",
    "                      'Tr':'Trail',\n",
    "                      'Trl':'Trail',\n",
    "                      'Ln': 'Lane',\n",
    "                     'Wy':'Way',\n",
    "                     }\n",
    "\n",
    "def correct_streetname(streetname):\n",
    "    \"\"\"Returns a corrected streetname\"\"\"\n",
    "    \n",
    "    #Capitalize all words in streetname string, remove any '.'s \n",
    "    streetname_capitalized = string.capwords(streetname).replace('.', '')\n",
    "\n",
    "    #Grab just the type of street (ie Avenue, or Ave)\n",
    "    street_type = street_type_re.search(streetname_capitalized)\n",
    "    \n",
    "    #Provided there is a street type, correct if not formatted as expected. \n",
    "    if street_type is not None:\n",
    "        street_type = street_type.group()\n",
    "        if street_type not in expected_streetnames:\n",
    "            if street_type in streetname_mapping:\n",
    "                new_street_type = streetname_mapping[street_type]\n",
    "                streetname = streetname_capitalized.replace(street_type, new_street_type)\n",
    "            else:\n",
    "                streetname = streetname_capitalized\n",
    "                logging.info(\"No streetname mapping correction: %s\", streetname)\n",
    "        else:\n",
    "            streetname = streetname_capitalized\n",
    "    else:\n",
    "        logging.info('street_type is NONE with this streetname: %s', streetname)\n",
    "    #TODO Add support for Spanish Street Prefixes\n",
    "    return streetname\n",
    "\n",
    "   \n",
    "def correct_zipcode(zipcode):\n",
    "    \"\"\"Returns a corrected zipcode\"\"\"\n",
    "    if is_postcode_re.search(zipcode) is None:\n",
    "        if is_postcode_plus_four_re.match(zipcode) is not None:\n",
    "            zipcode =  zipcode[:5]\n",
    "        elif zipcode[:3].upper() == 'CA ':\n",
    "            zipcode = zipcode[3:]\n",
    "            if is_postcode_re.search(zipcode) is not None:\n",
    "                return zipcode\n",
    "            else:\n",
    "                zipcode = ''\n",
    "        elif is_postcode_re.search(zipcode.replace(' ', '')) is not None:\n",
    "            return zipcode.replace(' ', '')\n",
    "        else:\n",
    "            zipcode = ''\n",
    "    return zipcode\n",
    "\n",
    "\n",
    "def is_zip_in_california(zipcode):\n",
    "    \"\"\"Checks if the zipcode could be in California\"\"\"\n",
    "    #First three digits of zip should be between 900 and 961 \n",
    "    if zipcode is not '':\n",
    "        try:\n",
    "            if 900 <= int(zipcode[:3]) <= 961:\n",
    "                return True\n",
    "            else:\n",
    "                logging.info('Non CA zip code found: %s', zipcode)\n",
    "                return False\n",
    "        except:\n",
    "            logging.info(\"Can't check for Califonia: \", zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "#######\n",
    "\n",
    "with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "    nodes_writer = UnicodeDictWriter(nodes_file, NODE_COLUMNS)\n",
    "    node_tags_writer = UnicodeDictWriter(nodes_tags_file, TAG_COLUMNS)\n",
    "    ways_writer = UnicodeDictWriter(ways_file, WAY_COLUMNS)\n",
    "    way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_COLUMNS)\n",
    "    way_tags_writer = UnicodeDictWriter(way_tags_file, TAG_COLUMNS)\n",
    "\n",
    "    nodes_writer.writeheader()\n",
    "    node_tags_writer.writeheader()\n",
    "    ways_writer.writeheader()\n",
    "    way_nodes_writer.writeheader()\n",
    "    way_tags_writer.writeheader()\n",
    "\n",
    "    for element in get_element(filename):\n",
    "        element_to_write = new_shape_element(element)\n",
    "        if 'tags' in element_to_write:\n",
    "            for tag in element_to_write['tags']:\n",
    "                if tag['key'] == 'street':\n",
    "                    tag['value'] = correct_streetname(tag['value'])\n",
    "                if tag['key'] == 'postcode':\n",
    "                    new_zip = correct_zipcode(tag['value'])\n",
    "                    if is_zip_in_california(new_zip):\n",
    "                        tag['value'] = new_zip\n",
    "                    else: \n",
    "                        tag['value'] = ''\n",
    "\n",
    "        if 'node' in element_to_write:\n",
    "            nodes_writer.writerow(element_to_write['node'])\n",
    "            if 'tags' in element_to_write:\n",
    "                node_tags_writer.writerows(element_to_write['tags'])\n",
    "        if 'way' in element_to_write:\n",
    "            ways_writer.writerow(element_to_write['way'])\n",
    "            if 'way_nodes' in element_to_write:\n",
    "                way_nodes_writer.writerows(element_to_write['way_nodes'])\n",
    "            if 'tags' in element_to_write:\n",
    "                way_tags_writer.writerows(element_to_write['tags']) \n",
    "\n",
    "logging.info('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streetname mapping was created iteratively with the help of LA County's helpful [Street Name Policy](https://dpw.lacounty.gov/services/roads/streetname/policy.pdf). Much of the time I would sample a subset of the data and see what didn't conform to my expectations to add new rules for correcting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SQL Exploration\n",
    "================\n",
    "\n",
    "First, let's find out some basic info about the data.\n",
    "\n",
    "__Number of users who contributed nodes:__\n",
    "\n",
    "    SELECT COUNT(DISTINCT user) FROM nodes;\n",
    "*342*\n",
    "\n",
    "\n",
    "__Number of Nodes, Ways, and Node Tags:__\n",
    "\n",
    "    SELECT COUNT(id) FROM nodes;\n",
    "*24,186,798*\n",
    "    \n",
    "    SELECT COUNT(id) FROM ways;\n",
    "*2,443,996*\n",
    "    \n",
    "    SELECT COUNT(*) FROM nodes_tags;\n",
    "*938160*\n",
    "\n",
    "__Number of nodes with tag information:__\n",
    "    \n",
    "    SELECT COUNT(DISTINCT key) FROM nodes_tags;\n",
    "*817*\n",
    "\n",
    "__Top 10 keys (labels for the value column) from nodes_tags:__\n",
    "\n",
    "    SELECT COUNT(key), key\n",
    "    FROM nodes_tags\n",
    "    GROUP BY key\n",
    "    ORDER BY COUNT(key) DESC LIMIT 10;\n",
    "|Number| Key|\n",
    "|----|-----|\n",
    "|89456|highway|\n",
    "|86950|source|\n",
    "|76347|street|\n",
    "|76264|housenumber|\n",
    "|72844|city|\n",
    "|72415|postcode|\n",
    "|40172|power|\n",
    "|40154|building|\n",
    "|38550|natural|\n",
    "|34357|country|\n",
    "\n",
    "I also took a look at the top 20 and found a key called 'amenity' which I will explore later.\n",
    "\n",
    "The key 'natural' looks interesting, let's see what natural features OSM users are adding. \n",
    "\n",
    "    SELECT COUNT(value), value\n",
    "    FROM nodes_tags\n",
    "    WHERE key LIKE â€˜naturalâ€™\n",
    "    GROUP BY value\n",
    "    ORDER BY COUNT(value) DESC LIMIT 10;\n",
    "\n",
    "|COUNT(value) | value|\n",
    "|-----|-----|\n",
    "|37572|tree|\n",
    "|568|peak|\n",
    "|108|bay|\n",
    "|93|spring|\n",
    "|54|cave_entrance|\n",
    "|34|beach|\n",
    "|19|bush|\n",
    "|19|succulent plant|\n",
    "|10|stone|\n",
    "|9|saddle|\n",
    "\n",
    "Well, the LA area certainly has more cave entrances than I expected! Very cool. However, the number of beaches seems low, I would like to see how they're being labeled. \n",
    "\n",
    "Let's explore the 'Amenities' Key in the same way. \n",
    "\n",
    "    SELECT COUNT(value), value FROM nodes_tags \n",
    "    WHERE key = â€˜amenityâ€™ \n",
    "    GROUP BY value\n",
    "    ORDER BY COUNT(value) DESC LIMIT 10;\n",
    "\n",
    "\n",
    "|COUNT(value) | value|\n",
    "|-----|-----|\n",
    "|3697|place_of_worship|\n",
    "|2877|school|\n",
    "|2620|restaurant|\n",
    "|1422|fast_food|\n",
    "|823|fuel|\n",
    "|787|bench|\n",
    "|758|cafe|\n",
    "|694|fountain|\n",
    "|589|toilets|\n",
    "|520|drinking_water|\n",
    "\n",
    "Being a coffee lover, the 758(!!!) cafes jump out at me. \n",
    "\n",
    "Let's get a little more complicated and see what zip codes have the most cafes. I'm betting on Silverlake or Los Feliz!\n",
    "\n",
    "    select count(nodes_tags.value), i.value\n",
    "    from nodes_tags\n",
    "    JOIN (select distinct(id), value, key FROM nodes_tags WHERE key='postcode') i\n",
    "    ON nodes_tags.id=i.id\n",
    "    WHERE nodes_tags.value='cafe'\n",
    "    GROUP BY i.value\n",
    "    ORDER BY count(nodes_tags.value) DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "|Number | Zip |\n",
    "|-----|-----|\n",
    "|7|90024|\n",
    "|5|90027|\n",
    "|5|92672|\n",
    "|4|90029|\n",
    "|4|90291|\n",
    "\n",
    "Seems like my intuition was only partially correct. Westwood actually came in #1 with 7 cafes, followed by Los Feliz and San Clemente - which also gives you an idea for how far south the dataset extends. \n",
    "\n",
    "It should be noted that zip codes aren't a great measure for cafe density because they are arbitrarily drawn and have sizes that vary. For a more thorough exploration of cafes in LA I would create a heatmap using the latitude and longitudes, make certain that there weren't cafes listed as restaurants in this dataset, and scrape new data from aggregators like Yelp. \n",
    "\n",
    "Finally, let's look at the top five sources for the node's and way's tags.\n",
    "\n",
    "    SELECT COUNT(*), value \n",
    "    FROM (SELECT * FROM nodes_tags UNION ALL \n",
    "          SELECT * FROM ways_tags)\n",
    "    WHERE key = 'source'\n",
    "    GROUP BY value\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "|COUNT | value |\n",
    "|-----|-----|\n",
    "|108353|tiger_import_dch_v0.6_20070809|\n",
    "|51774|bing_imagery|\n",
    "|15751|\"SanGIS Addresses Public Domain (http://www.sangis.org/)\"|\n",
    "|11738|Yahoo|\n",
    "|10867|Bing|\n",
    "\n",
    "This makes sense with some of the things we saw while cleaning, like the many datapoints coming from the Tiger systems.\n",
    "\n",
    "### Ideas for Additional Improvement\n",
    "\n",
    "After becoming so familiar with this data I have begun to wonder if there isn't a way to make sure the data is normalized going into the OSM system. Perhaps there are technical barriers to this, but ensuring that certain values are entered uniformly (5 digit zips being a simple example) would make large scale analyses much easier. \n",
    "\n",
    "One potential issue with this idea is making a service that is largely kept update by volunteers harder to use, therefore makeing contributors less likely to loan their time. \n",
    "\n",
    "### Summary\n",
    "\n",
    "Dealing with a large (for me) dataset where much of the information was typed by humans was illuminating - I now know why there are so many articles that mention how much of a data professional's time is spent *cleaning* data. \n",
    "\n",
    "It was great to parse xml iteratively in python (instead of reading the whole document into memory), and exploring with SQL was an excellent experience - very different from what I've done with pandas in the past. I've now done many, many SQL tutorials and it's left me wanting  more - the chance to answer real business questions perhaps, or to explore how to go back and forth between SQL and visualization tools. I feel that my firm grip on relational databases will be important as the data I deal with grows more and more complex.\n",
    "\n",
    "And, in the meantime, I've found a couple hundred fixes I can contribute to the OSM project!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
